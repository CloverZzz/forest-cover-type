# Illustrative examples of Machine Learning Techniques Applied to Kaggle Forest Cover Data

A complete machine learning project pipeline is presented for the Kaggle Forest Cover Type competition.  The individual techniques are illustrative examples and can be used as the basis for further discussion.

## Files

JonathanHull_capstone.ipynb -- Jupyter notebook with source code

best_gbrt_cfl.pkl -- optimized Gradient Boosting classifier

best_mlp52_19_7_clf.pkl	 -- optimized multi-layer perceptron

best_mlp_clf.pkl -- basic multi-layer perceptron

best_rf_clf.pkl	-- optimized random forest classifier

best_sgd_clf.pkl -- optimized stochastic gradient descent classifier

kaggle_test.csv	-- test data from the kaggle competition

kaggle_train.csv -- training data from the kaggle competition

submission_best_rf.csv	-- kaggle submission with results from best RF classifier

submission_majority_vote.csv -- kaggle submission with majority vote results

submission_stacked.csv -- kaggle submission with one level of stacking with logistic regression

## Installation

Copy the Jupyter notebook JonathanHull_capstone.ipynb as well as the data files in \*.csv to your python environment and open the notebook.  You can begin exceuting the cells from the top.  Everything should work seemlessly as long as you have the typical pandas, numpy, etc. installed.

## Code

## Run

## Data
### Features
### Target Variable
