# Illustrative examples of Machine Learning Techniques Applied to Kaggle Forest Cover Data

A complete machine learning project pipeline is presented for the Kaggle Forest Cover Type competition.  The individual techniques are illustrative examples and can be used as the basis for further discussion.

## Files

plots/ -- png files from the Jupyter notebook for the report

proposal/ -- project proposal

submission/ -- files submitted for final evaluation

JonathanHull_capstone.ipynb -- Jupyter notebook with source code

best_gbrt_cfl.pkl -- optimized Gradient Boosting classifier

best_mlp52_19_7_clf.pkl	 -- optimized multi-layer perceptron

best_mlp_clf.pkl -- basic multi-layer perceptron

best_rf_clf.pkl	-- optimized random forest classifier

best_sgd_clf.pkl -- optimized stochastic gradient descent classifier

kaggle_test.csv	-- test data from the kaggle competition

kaggle_train.csv -- training data from the kaggle competition

submission_best_rf.csv	-- kaggle submission with results from best RF classifier

submission_majority_vote.csv -- kaggle submission with majority vote results

submission_stacked.csv -- kaggle submission with one level of stacking with logistic regression

## Installation

Copy the Jupyter notebook JonathanHull_capstone.ipynb as well as the data files in \*.csv to your python environment and open the notebook.  You can begin exceuting the cells from the top.  Everything should work seemlessly as long as you have the typical pandas, numpy, etc. installed.

This project requires Python 3.0+ and the following Python libraries:

NumPy
Pandas
matplotlib
scikit-learn

You will also need to have software installed to run and execute an iPython Notebook

We recommend you install Anaconda, a pre-packaged Python distribution that contains all of the necessary libraries and software for this project.

## Code

Python code for the project is in JonathanHull_capstone.ipynb.

## Run

## Data
### Features
### Target Variable
